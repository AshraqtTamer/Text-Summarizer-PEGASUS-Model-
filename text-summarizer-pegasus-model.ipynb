{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-04T19:41:39.925863Z","iopub.execute_input":"2026-01-04T19:41:39.926484Z","iopub.status.idle":"2026-01-04T19:41:40.225649Z","shell.execute_reply.started":"2026-01-04T19:41:39.926460Z","shell.execute_reply":"2026-01-04T19:41:40.224908Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### üöÄ PEGASUS Model Overview\n\n- **üß† Type:** Encoder-Decoder (Sequence-to-Sequence) Transformer  \n- **üìö Pretraining:** Unlabeled text for abstractive summarization. Recent work uses [PEGASUS](https://huggingface.co/papers/1912.08777) to pre-train on massive text corpora with self-supervised objectives.  \n- **üéØ Objective Functions:**  \n  - üìù Masked Language Modeling (MLM)  \n  - üîç Gap Sentence Generation (GSG) ‚Äî whole sentences are masked and the model predicts them  \n- **‚ö° Fine-Tuning:** Performs well even on small datasets (as few as 1000 examples)  \n- **üíæ Checkpoints:** Original Pegasus checkpoints are available under the Google organization  \n","metadata":{}},{"cell_type":"markdown","source":"### üõ†Ô∏è Tools & Technologies Used\n\n- **üêç Python** ‚Äì Main programming language for the project  \n- **ü§ó Transformers Library** ‚Äì `transformers` by Hugging Face, used to load pre-trained PEGASUS models and pipelines  \n- **üöÄ PEGASUS Model** ‚Äì Pre-trained encoder-decoder Transformer for abstractive summarization (model: `google/pegasus-cnn_dailymail`)   \n- **üé® Gradio** ‚Äì Creates a simple and interactive web interface for input and output  \n- **üíª Kaggle Notebooks** ‚Äì Environments to run and test the app interactively  \n","metadata":{}},{"cell_type":"code","source":"# -------------------------------\n# 1. Install & Import Libraries\n# -------------------------------\n\n\nfrom transformers import pipeline\n\n# -------------------------------\n# 2. Load Pretrained Summarization Model\n# -------------------------------\n\n# Pegasus model trained on CNN/DailyMail dataset ‚Äî great for news-style summaries\nsummarizer = pipeline(\"summarization\",model=\"google/pegasus-cnn_dailymail\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T19:42:56.885555Z","iopub.execute_input":"2026-01-04T19:42:56.886281Z","iopub.status.idle":"2026-01-04T19:43:05.695249Z","shell.execute_reply.started":"2026-01-04T19:42:56.886252Z","shell.execute_reply":"2026-01-04T19:43:05.694653Z"}},"outputs":[{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nDevice set to use cuda:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def summarize_text(text):\n    summary = summarizer(text, max_length=200, min_length=30, do_sample=False)\n    return summary[0]['summary_text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T19:53:26.381051Z","iopub.execute_input":"2026-01-04T19:53:26.381333Z","iopub.status.idle":"2026-01-04T19:53:26.385412Z","shell.execute_reply.started":"2026-01-04T19:53:26.381312Z","shell.execute_reply":"2026-01-04T19:53:26.384544Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# -------------------------------\n# 3. Create Gradio Interface\n# -------------------------------\nimport gradio as gr\nimport re\n\niface = gr.Interface(\n    fn=summarize_text,\n    inputs=gr.Textbox(\n        lines=15, \n        placeholder=\"üìù Enter your text here...\", \n        label=\"Input Text\"\n    ),\n    outputs=gr.Textbox(\n        lines=10,\n        placeholder=\"Summary will appear here...\",\n        label=\"Summary\"\n    ),\n    title=\"üöÄ PEGASUS Summarizer\",\n    description=\"\"\"\n    **Summarize long text quickly!** ‚ú®  \n    Enter any text and get an **abstractive summary** using the [PEGASUS](https://huggingface.co/papers/1912.08777) model.  \n    Works well even for **long paragraphs and articles**. üì∞üìö\n    \"\"\",\n)\n\n# -------------------------------\n# 4. Launch the App\n# -------------------------------\niface.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T20:09:44.415117Z","iopub.execute_input":"2026-01-04T20:09:44.415700Z","iopub.status.idle":"2026-01-04T20:09:45.339446Z","shell.execute_reply.started":"2026-01-04T20:09:44.415655Z","shell.execute_reply":"2026-01-04T20:09:45.338830Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7866\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://4d592fdae74dda92e5.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://4d592fdae74dda92e5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}